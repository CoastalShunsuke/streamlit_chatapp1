import streamlit as st
import os
from dotenv import load_dotenv
import pinecone  # ã“ã“ã‚’ä¿®æ­£
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone as LangChainPinecone
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# ...ï¼ˆçœç•¥ï¼‰...

# Pinecone ã®åˆæœŸåŒ–
if pinecone_api_key:
    try:
        # Pineconeã®åˆæœŸåŒ–ã‚’è¿½åŠ 
        pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)

        if index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=index_name,
                dimension=1536,
                metric="cosine",
            )
            st.success(f"æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"Pinecone åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.error("Pinecone API Key ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state["history"] = []

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹
st.subheader("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
user_input = st.text_input("è³ªå•", placeholder="ä¾‹: æœ€è¿‘ã®AIæŠ€è¡“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ï¼Ÿ")

if user_input and openai_api_key and pinecone_api_key:
    # OpenAI Embeddingsã®åˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # **Pineconeã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€å‰ã«åˆæœŸåŒ–ãŒå¿…è¦**
    # Pineconeã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
    vector_store = LangChainPinecone.from_existing_index(
        index_name=index_name, embedding=embeddings
    )

    # OpenAI LLM ã®åˆæœŸåŒ–
    llm = OpenAI(openai_api_key=openai_api_key, temperature=temperature)

    # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’ã¾ã¨ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    conversation_history = "\n".join(
        [f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {assistant}" for user, assistant in st.session_state["history"]]
    )

    # ç¾åœ¨ã®è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    if conversation_history:
        prompt = f"{conversation_history}\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"
    else:
        prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"

    # RetrievalQA ã®ä½œæˆ
    retriever = vector_store.as_retriever(search_kwargs={"k": top_k})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, retriever=retriever, chain_type="stuff", return_source_documents=True
    )

    # è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¨å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã‚’å–å¾—
    result = qa_chain({"query": prompt})

    response = result['result']
    source_documents = result['source_documents']

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è³ªå•ã¨å›ç­”ã‚’è¿½åŠ 
    st.session_state["history"].append((user_input, response))

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    with st.expander("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´", expanded=True):
        for i, (user, assistant) in enumerate(st.session_state["history"]):
            st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼ ({i+1})**: {user}")
            st.markdown(f"**ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ**: {assistant}")

    # å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®è¡¨ç¤º
    st.subheader("ğŸ” å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®ä¸€éƒ¨:")
    for doc in source_documents:
        st.markdown(f"**æ–‡æ›¸å†…å®¹**: {doc.page_content}")
