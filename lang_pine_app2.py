import streamlit as st
import os
import pinecone
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’å…ƒã«æˆ»ã™
from langchain.vectorstores import Pinecone as LangChainPinecone
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.set_page_config(page_title="Conversational RAG Chatbot", page_icon="ğŸ’¬")
st.title("ğŸ’¬ Conversational RAG Chatbot")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸRetrieval-Augmented Generation (RAG) ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
Pinecone ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¨ OpenAI ã® GPT ã‚’çµ„ã¿åˆã‚ã›ã€éå»ã®å±¥æ­´ã‚’è¸ã¾ãˆã¦å¯¾è©±ã‚’è¡Œã„ã¾ã™ã€‚
""")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
st.sidebar.title("ğŸ›  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
temperature = st.sidebar.slider("Temperature (ç”Ÿæˆã®å¤šæ§˜æ€§)", 0.0, 1.0, 0.7, step=0.1)
top_k = st.sidebar.slider("Top-k Documents (æ¤œç´¢æ™‚ã®ä¸Šä½æ–‡æ›¸æ•°)", 1, 10, 3)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
openai_api_key = st.secrets["OPENAI_API_KEY"]
pinecone_api_key = st.secrets["PINECONE_API_KEY"]
pinecone_env = st.secrets["PINECONE_ENVIRONMENT"]
index_name = st.secrets["PINECONE_INDEX_NAME"]

# Pinecone ã®åˆæœŸåŒ–
if pinecone_api_key:
    try:
        # Pinecone ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        pc = Pinecone(api_key=pinecone_api_key)

        if index_name not in [index.name for index in pc.list_indexes()]:
            pc.create_index(
                name=index_name,
                dimension=1536,
                metric="cosine",
                spec=ServerlessSpec(
                    cloud='aws',
                    region=pinecone_env
                )
            )
            st.success(f"æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"Pinecone åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.error("Pinecone API Key ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")


# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state["history"] = []

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹
st.subheader("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
user_input = st.text_input("è³ªå•", placeholder="ä¾‹: æœ€è¿‘ã®AIæŠ€è¡“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ï¼Ÿ")

if user_input and openai_api_key and pinecone_api_key:
    # OpenAI Embeddings ã®åˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # Pinecone ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    index = pc.Index(index_name)

    # LangChain ã® Pinecone ã‚’åˆæœŸåŒ–
    vector_store = LangChainPinecone(index, embeddings.embed_query)

    # OpenAI LLM ã®åˆæœŸåŒ–
    llm = OpenAI(openai_api_key=openai_api_key, temperature=temperature)

    # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’ã¾ã¨ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    conversation_history = "\n".join(
        [f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {assistant}" for user, assistant in st.session_state["history"]]
    )

    # ç¾åœ¨ã®è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
    if conversation_history:
        prompt = f"{conversation_history}\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"
    else:
        prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"

    # RetrievalQA ã®ä½œæˆ
    retriever = vector_store.as_retriever(search_kwargs={"k": top_k})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, retriever=retriever, chain_type="stuff", return_source_documents=True
    )

    # è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¨å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã‚’å–å¾—
    result = qa_chain({"query": user_input})

    response = result['result']
    source_documents = result['source_documents']

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è³ªå•ã¨å›ç­”ã‚’è¿½åŠ 
    st.session_state["history"].append((user_input, response))

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    with st.expander("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´", expanded=True):
        for i, (user, assistant) in enumerate(st.session_state["history"]):
            st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼ ({i+1})**: {user}")
            st.markdown(f"**ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ**: {assistant}")

    # å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®è¡¨ç¤º
    st.subheader("ğŸ” å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®ä¸€éƒ¨:")
    for doc in source_documents:
        st.markdown(f"**æ–‡æ›¸å†…å®¹**: {doc.page_content}")
