import streamlit as st
import os
import pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone as LangChainPinecone
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.set_page_config(page_title="Conversational RAG Chatbot", page_icon="ğŸ’¬")
st.title("ğŸ’¬ Conversational RAG Chatbot")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸ Retrieval-Augmented Generation (RAG) ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
Pinecone ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¨ OpenAI ã® GPT ã‚’çµ„ã¿åˆã‚ã›ã€éå»ã®å±¥æ­´ã‚’è¸ã¾ãˆã¦å¯¾è©±ã‚’è¡Œã„ã¾ã™ã€‚
""")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
st.sidebar.title("ğŸ›  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
temperature = st.sidebar.slider("Temperature (ç”Ÿæˆã®å¤šæ§˜æ€§)", 0.0, 1.0, 0.7, step=0.1)
top_k = st.sidebar.slider("Top-k Documents (æ¤œç´¢æ™‚ã®ä¸Šä½æ–‡æ›¸æ•°)", 1, 10, 3)

# ç’°å¢ƒå¤‰æ•°ã®å–å¾—ï¼ˆst.secrets ã‚’ä½¿ç”¨ï¼‰
openai_api_key = st.secrets["OPENAI_API_KEY"]
pinecone_api_key = st.secrets["PINECONE_API_KEY"]
pinecone_env = st.secrets["PINECONE_ENVIRONMENT"]
index_name = st.secrets["PINECONE_INDEX_NAME"]

# Pinecone ã®åˆæœŸåŒ–
if pinecone_api_key:
    try:
        pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)
        if index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=index_name,
                dimension=1536,
                metric="cosine",
            )
            st.success(f"æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"Pinecone åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.error("Pinecone API Key ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹
st.subheader("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
user_input = st.text_input("è³ªå•", placeholder="ä¾‹: æœ€è¿‘ã®AIæŠ€è¡“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ï¼Ÿ")

if user_input and openai_api_key and pinecone_api_key:
    # OpenAI Embeddings ã®åˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # Pinecone ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
    vector_store = LangChainPinecone.from_existing_index(
        index_name=index_name, embedding=embeddings
    )

    # OpenAI Chat Model ã®åˆæœŸåŒ–
    llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=temperature)

    # Conversational Retrieval Chain ã®ä½œæˆ
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": top_k}),
        return_source_documents=True
    )

    # ä¼šè©±å±¥æ­´ã‚’å–å¾—
    chat_history = st.session_state["chat_history"]

    # è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¨å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã‚’å–å¾—
    result = qa_chain({"question": user_input, "chat_history": chat_history})

    response = result["answer"]
    source_documents = result["source_documents"]

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è³ªå•ã¨å›ç­”ã‚’è¿½åŠ 
    chat_history.append((user_input, response))
    st.session_state["chat_history"] = chat_history

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    with st.expander("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´", expanded=True):
        for i, (user, assistant) in enumerate(chat_history):
            st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼ ({i+1})**: {user}")
            st.markdown(f"**ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ**: {assistant}")

    # å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®è¡¨ç¤º
    st.subheader("ğŸ” å‚ç…§ã•ã‚ŒãŸæ–‡æ›¸ã®ä¸€éƒ¨:")
    for doc in source_documents:
        st.markdown(f"**æ–‡æ›¸å†…å®¹**: {doc.page_content}")
